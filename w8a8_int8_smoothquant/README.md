# ðŸ“¦ `run_compress.py`

End-to-end compression for **GLM-4.5-Air** that

1. **SmoothQuant 0.8**  
2. **GPTQ INT8 (W8A8)** post-training quantisation  
3. **2 : 4 structured sparsity** (50 %)

while **router, embeddings, LayerNorms, gate/up/down projections and `lm_head` remain FP16**.  
The resulting checkpoint loads straight into **vLLM** or **TensorRT-LLM**.

---

## âœ¨ Pipeline at a glance

| # | Modifier | Purpose |
|---|----------|---------|
| 1 | `SmoothQuantModifier` | Balance activation/weight ranges â†’ INT8 accuracy |
| 2 | `GPTQModifier(scheme="W8A8")` | 8-bit weights **+** 8-bit activations |
| 3 | `SparseGPTModifier(pattern="2:4", sparsity=0.5)` | Enforce NVIDIA 2 : 4 sparsity for cuSPARSELt/TRT-LL speed-ups |

*Calibration* â†’ **WikiText-103** (512 samples)  
*Output*     â†’ `<output_dir>` (weights + tokenizer)

---

## ðŸš€ Quick start

### Minimum viable GPU rig â€” **4 Ã— RTX 3090 (24 GB each)**

```bash
# 0.  Core deps (CUDA 12.8 wheels shown â€” tweak for your system)
pip install torch==2.3.0+cu128 transformers>=4.44 vllm>=0.4.2 \
            llmcompressor>=0.3.0 accelerate>=0.28

# 1.  Compress
export CUDA_VISIBLE_DEVICES=0,1,2,3            # optional pinning
python run_compress.py \
  zai-org/GLM-4.5-Air \
  --output_dir ./glm45-air-w8a8-s24-routerfp16 \
  --tensor_parallel_size 4 \
  --num_calibration_samples 512

# 2.  Serve with vLLM
python -m vllm.entrypoints.openai.api_server \
  --model ./glm45-air-w8a8-s24-routerfp16 \
  --tensor-parallel-size 4 \
  --port 8000
````

<details>
<summary>ðŸ“Š VRAM budget per 24 GB card (TP = 4)</summary>

| Component            |  Size / GPU | Notes                       |
| -------------------- | ----------: | --------------------------- |
| Weights (INT8 + 2:4) | **13.3 GB** | 106 B Ã— 1 byte Ã— 0.5 Ã· 4    |
| GPTQ meta            |    \~0.3 GB | scales / zeros              |
| KV-cache\*           |      7-8 GB | 16 k context, batch 1       |
| **Free**             |  **â‰ˆ 2 GB** | activations + fragmentation |

\* hidden = 8 192, layers = 32.  Larger batch or context â‡’ add GPUs or move KV off-GPU.

</details>

---

## âš™ï¸ Valid tensor-parallel sizes (up to 8 GPUs)

All shardable dims (`hidden_size = 4096`, `heads = 96`, `experts = 128`) are divisible by **1, 2, 4, 8**.
On 24 GB cards, memory constraints rule out TP = 1 & 2:

| TP    | GPUs  | Weights / GPU | Fits 24 GB? | Comment                            |
| ----- | ----- | ------------- | ----------- | ---------------------------------- |
| 1     | 1     | 53 GB         | âŒ           | Needs â‰¥ 48 GB (A100-80G, H100-80G) |
| 2     | 2     | 26.5 GB       | âŒ           | Drop to W4A8 **or** add GPUs       |
| **4** | **4** | **13.3 GB**   | **âœ…**       | Recommended minimum                |
| **8** | **8** | **6.6 GB**    | **âœ…**       | Ample head-room                    |

> **Rule of thumb**â€ƒRequired VRAM â‰ˆ **weights Ã· TP + KV-cache**.

---

## ðŸ“ Context-length head-room (KV-cache footprint)

For FP16 keys + values the per-token cost:

```
bytes_per_token = hidden_size Ã— 2 bytes Ã— 2 (K+V) Ã— num_layers
                = 4096 Ã— 2 Ã— 2 Ã— 32  â‰ˆ 524 288 B  (â‰ˆ 0.5 MB)
```

Dividing by TP:

| TP    | Bytes / token / GPU |   â‰ˆ MB @ 8 k |  â‰ˆ MB @ 16 k |  â‰ˆ MB @ 32 k |
| ----- | ------------------: | -----------: | -----------: | -----------: |
| **4** |           131 072 B | **1 024 MB** | **2 048 MB** | **4 096 MB** |
| **8** |            65 536 B |   **512 MB** | **1 024 MB** | **2 048 MB** |

### Context limits on 24 GB cards

| GPUs (TP)    | Weights / GPU | Max ctx (batch = 1)\*  | Free VRAM |
| ------------ | ------------: | ---------------------- | --------- |
| **4 Ã— 3090** |   **13.3 GB** | **â‰¤ 32 k** (â‰ˆ 4 GB KV) | \~6 GB    |
| **8 Ã— 3090** |    **6.6 GB** | **â‰¤ 64 k** (â‰ˆ 4 GB KV) | \~13 GB   |

\* For batch > 1 use `total_KV = batch Ã— context Ã— bytes_per_token Ã· TP`.

---

## ðŸ› ï¸ CLI reference

| Flag                        | Default | Description                               |
| --------------------------- | ------- | ----------------------------------------- |
| `model_id`                  | â€”       | HF repo or local FP16 path                |
| `--output_dir`              | â€”       | Where to write the compressed checkpoint  |
| `--tensor_parallel_size`    | 1       | TP world-size for compression & inference |
| `--num_calibration_samples` | 512     | WikiText samples for PTQ                  |
| `--max_seq_length`          | 2048    | Max tokens during calibration             |

---

## ðŸ“‚ Output layout

```
glm45-air-w8a8-s24-routerfp16/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin      # INT8 + sparse
â”œâ”€â”€ tokenizer.json
â””â”€â”€ special_tokens_map.json
```

---

## ðŸ–¥ï¸ Hardware cheat-sheet

| GPUs         | VRAM / card | TP | Fits W8A8 + 2:4? | Context head-room (bs = 1) |
| ------------ | ----------- | -- | ---------------- | -------------------------- |
| **4 Ã— 3090** | 24 GB       | 4  | âœ…                | â‰¤ 32 k                     |
| 4 Ã— 4090     | 24 GB       | 4  | âœ…                | â‰¤ 32 k                     |
| **8 Ã— 3090** | 24 GB       | 8  | âœ…                | â‰¤ 64 k                     |
| 4 Ã— A800-80G | 80 GB       | 4  | âœ…                | 100 k +                    |
| 1 Ã— H100-80G | 80 GB       | 1  | âœ…                | â‰¤ 16 k (single-GPU TP = 1) |

---

## ðŸ”§ Tweak the recipe

* Adjust `SmoothQuantModifier(smoothing_strength=â€¦)` (0 â†’ off, 1 â†’ aggressive).
* Switch `GPTQModifier` to `scheme="W4A8"` to halve weight size (fits 2 Ã— 3090).
* Increase `sparsity` if your kernels exploit > 50 % 2 : 4.

---

## â“ Troubleshooting

| Issue                | Likely cause           | Fix                                         |
| -------------------- | ---------------------- | ------------------------------------------- |
| CUDA OOM at load     | Not enough GPUs        | Use â‰¥ 4 cards or drop to W4A8               |
| Accuracy drop > 1 pp | Over-sparse or high SQ | Lower `sparsity` or `smoothing_strength`    |
| No TRT-LLM speed-up  | 2 : 4 kernels missing  | Re-build TRT-LLM with `SPARSITY_ENABLED=ON` |

---

## ðŸ“œ License

Apache-2.0 (same as [`llmcompressor`](https://github.com/Mikubill/LLM-COMPRESSOR) & [`vllm`](https://github.com/vllm-project/vllm))
Â© 2025 Thomas Whitworth & Contributors

```
::contentReference[oaicite:0]{index=0}
```
